# -*- coding: utf-8 -*-
"""KMEANS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AHOdj1relbHJfcZTYRxcI4ACVg6qkQug

mounting the drive to collabnotebook
"""

from google.colab import drive
drive.mount("/content/drive/")

"""Import necessary libraries:"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""Load the dataset:"""

path="/content/drive/MyDrive/Colab Notebooks/GSE20347_Disease.csv"
data=pd.read_csv(path)
data.describe

"""Checking Null values"""

# Check if there are any null values in the entire DataFrame
any_null_values = data.isnull().sum().any()

if any_null_values:
    print("There are null values in the DataFrame.")
else:
    print("There are no null values in the DataFrame.")

"""# Data Preprocessing
# Assuming 'ID_REF' column contains unique identifiers and is not used for clustering
"""

data_numeric = data.drop('ID_REF', axis=1)
data_numeric = data_numeric.replace(',', '', regex=True).astype(float)
data_numeric = data_numeric.fillna(data_numeric.mean())

"""Visualize Clusters (Optional):

# Apply PCA to reduce dimensionality
"""

from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming data_numeric is already prepared
# Apply PCA with the desired number of components (e.g., 17)
pca = PCA(n_components=17)
data_pca = pca.fit_transform(data_numeric)

# Create a DataFrame with the principal components
columns_pca = [f'PC{i+1}' for i in range(data_pca.shape[1])]
data_pca_df = pd.DataFrame(data_pca, columns=columns_pca)

# Display the explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratio:")
print(explained_variance_ratio)

# Plot the cumulative explained variance
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), marker='o', linestyle='--')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

# Display the explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratio:")
print(explained_variance_ratio)

# Identify the most significant component
most_significant_component = np.argmax(explained_variance_ratio) + 1  # Adding 1 to convert from zero-based indexing
print(f"The most significant component is PC{most_significant_component} with explained variance ratio: {explained_variance_ratio[most_significant_component - 1]:.4f}")

import matplotlib.pyplot as plt

# Assume explained_variance_ratio contains the explained variance ratio for each PC

# Plot the scree plot
plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')
plt.title('Scree Plot')
plt.xlabel('Principal Component Number')
plt.ylabel('Explained Variance Ratio')
plt.show()

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Assuming data_numeric is already prepared
# Apply PCA with the desired number of components (e.g., 17)
pca = PCA(n_components=17)
data_pca = pca.fit_transform(data_numeric)

# Selecting the first two principal components for clustering
subset_data = data_pca[:, :2]

# Scaling the data (important for K-means)
scaler = StandardScaler()
subset_data_scaled = scaler.fit_transform(subset_data)

# Applying K-means with, for example, 3 clusters
n_clusters = 2  # we can adjust the number of clusters based on our analysis
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
data['Cluster'] = kmeans.fit_predict(subset_data_scaled)

# Calculate Silhouette Score
silhouette_avg = silhouette_score(subset_data_scaled, data['Cluster'])
print("Silhouette Score:", silhouette_avg)

# Displaying the results
print(data['Cluster'].value_counts())

# You can visualize the clusters or further analyze them based on your specific goals



"""# storing the result in xl files

"""

# Assuming 'Cluster' column has been added to the 'data' DataFrame
# (as shown in the previous code examples)

# Specify the directory where you want to save the Excel file
output_directory = "/content/drive/MyDrive/Colab Notebooks/Cluster_Results2"

# Create the output directory if it doesn't exist
os.makedirs(output_directory, exist_ok=True)

# Create a Pandas Excel writer using XlsxWriter as the engine
excel_writer = pd.ExcelWriter(os.path.join(output_directory, 'cluster_results.xlsx'), engine='xlsxwriter')

# Iterate through each cluster and write to a separate sheet
for cluster_label in data['Cluster'].unique():
    # Select data for the current cluster
    cluster_data = data[data['Cluster'] == cluster_label]

    # Write the cluster data to a sheet with the cluster label
    cluster_data.to_excel(excel_writer, sheet_name=f'cluster_{cluster_label}', index=False)

excel_writer.save()
excel_writer.close()
